<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>My Personal Resume</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 20px auto;
            background-color: #fff;
            padding: 20px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        header {
            text-align: center;
        }
        header h1 {
            margin-bottom: 5px;
        }
        .contact-info {
            text-align: center;
            margin-bottom: 20px;
        }
        .contact-info p {
            margin: 5px;
        }
        section {
            margin-bottom: 30px;
        }
        section h2 {
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
            margin-bottom: 10px;
        }
        ul {
            list-style: none;
            padding: 0;
        }
        ul li {
            margin-bottom: 10px;
        }
        .skills-list {
            display: flex;
            flex-wrap: wrap;
        }
        .skills-list li {
            background-color: #eee;
            border-radius: 5px;
            padding: 10px;
            margin: 5px;
            flex: 1 1 calc(50% - 10px);
        }
        footer {
            text-align: center;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Ravi Shankar</h1>
            <h3>Data Engineer</h3>
        </header>

        <div class="contact-info">
            <p>Email: rldr.shankar@gmail.com</p>
            <p>Phone: +1 (337)-739-1082</p>
            <p>Website: rshankar.framer.ai</p>
        </div>

        <section>
            <h2>Summary</h2>
            <p>
                Meet Ravi Shankar, a seasoned professional with 9 years of industry expertise under his belt. A data wizard, with a powerful repertoire of ETL, Hadoop, AWS, Snowflake, and Python scripting.
            </p>
        </section>

        <section>
            <h2>Experience</h2>
            <ul>
                <li>
                    <h3><strong>Professional Work Summary</strong> 9 Years of Experience </h3>
                    • 9 years of experience in ETL and Hadoop with complete SDLC including requirement gathering, analysis, design, development, implementation and maintenance of Data warehouse application, also have experience with AWS, Snowflake, and writing python scripts.
                    <br /> • Experience in populating and maintaining Data warehouses and Data Marts using IBM Information Server 11.7, 11.5, 11.3, 9.1, v8.7, 8.5 (Administrator, Director, Manager, Designer).
                    <br /> • Extensive experience with SQL, PYSPARK, HIVE, PYTHON, Snowflake and Shell Scripting.
                    <br /> • Have used filters and functionalities that snowflake provides to optimize and set up the data pipeline.
                    <br /> • Experience working with Snowflake warehouse for loading and extracting data.
                    <br /> • Strong knowledge DWH concepts like Entity-Relationship Model, Facts, and dimensions, slowly changing dimensions (SCD) and Dimensional Modeling (Star and Snowflake Schemas)
                    <br /> • Experience in Designing and populating Dimensional model (Star & Snowflake Schema) for Data warehouse and data marts and Reporting Data Sources..
                    <br /> • Experience in using Snowflake Cloning, warehouse usages / multi-cluster warehouse and Time Travel features
                    <br /> • Progressive experience in field of Big Data Technologies such as Hadoop and Hive and knowledge of PySpark
                    <br /> • Experience in writing complex SQL queries involving multiple tables inner and outer joins, and writing views and procedures.
                    <br /> • Experience in scheduling DataStage jobs using Control M, CAWA (CA workload Automation) and DataStage Director 
                    <br /> • Extensively used DataStage Director for executing, monitoring data, analyzing logs.
                    <br /> • Experience with the design of large-scale ETL solutions integrating multiple source systems DB2, SQL Server, Oracle and Snowflake Databases.
                    <br /> • Experience with Hierarchical stage to parse data into JSON or XML to create POST and PUT requests to Rest API.
                    <br /> • Proven track record in troubleshooting of DataStage jobs and addressing production issues like performance tuning and enhancement.
                    <br /> • In depth experience in dealing with DataStage Designer stages like Lookup, Join, Merge, Row generator, Transformer, Remove Duplicate, Sort, Peek, Change capture, Filter, Copy, Sequential File, FTP, Data Set, ODBC, Snowflake connector, Hive connector etc.In depth experience in dealing with DataStage Designer stages like Lookup, Join, Merge, Row generator, Transformer, Remove Duplicate, Sort, Peek, Change capture, Filter, Copy, Sequential File, FTP, Data Set, ODBC, Snowflake connector, Hive connector etc.
                    <br /> • Experience with SAP Business Objects Data Integrator for ETL extraction, transformation and loading data from multiple source systems such as excel, SAP Hana, SQL and flat files.
                    <br /> • Strong experience in UNIX Shell as part of file manipulation and text processing
                    <br /> • Extensively using Talend components tMap, tDie, tConervterType, tFlowMeter, tLogCatcher, tRowGenerator, tOracleInput, tOracleOutput, tFileList, tDelimited and using Custom expressions etc.
                    <br /> • Hands on experience in working with Big Data Ecosystems like HDFS, Hive, Sqoop, Spark.
                    <br /> • Excellent communication, interpersonal, analytical skills, and strong ability to understand long term project development issues at all levels, with analytical and problem-solving skills.
                    <br /> • Experience in various methodologies like Waterfall and Agile.
                    <br /> • Prepare technical design and mapping documents & develop ETL data pipelines for error handling


                    <h3><strong>Companies: </strong> Clients </h3>
                    <ul class="skills-list">
                        <li> <strong>WorldPay: </strong> Senior Data Enginner (Apr 2024 - Current) </li>
                        <li> <strong>Toyota Financial Services (TFS): </strong> Senior Data Enginner (Dec 2023 - Apr 2024) </li> 
                        <li> <strong>FIS Global </strong> Data Enginner (Aug 2021 - Dec 2023) </li>
                        <li> <strong>DTCC: </strong> DataStage Developer (March 2021 - Aug 2021) </li>
                        <li> <strong>United Service Automobile Association (USAA): </strong> DataStage Developer (Feb 2020 - March 2021) </li>
                        <li> <strong>Gap:  </strong> DataStage Developer (Nov 2019 - Feb 2020) </li>
                        <li> <strong>CDI Solutions:  </strong> DataStage Developer and Data Engineer (Sept 2019 - Current) </li>

                    </ul>
                </li>
                <!-- <li>
                    <strong>Sr Data Engineer</strong> - Worldpay (April 2024 – Present)  
                    <br /> - Working on to analyze data from three different platforms to build a combined data mart for Revenue boost data   
                    <br /> - Created several queries to transform the three different platform’s data to sync up together  
                    <br /> - Created multiple Snowflake SQL and JavaScript procedure to define the ETL process
                    <br /> - Created Snowflake tasks to schedule the data loads in snowflake
                    <br /> - Worked and implemented the Salesforce procedures created the dashboard from the analyzed data
                    <br /> - Created snowflake streams to identify the record change and load the history
                </li>
                <li>
                    <strong>Sr Data Engineer</strong> - TFS – Toyota Financial Services (Dec 2023 – Apr 2024)  
                    <br /> - Designed data models and developed the Snowflake pipeline to ingest the data.
                    <br /> - Worked on Collection data to make sure, history data is available, and users are able to create reports based on that data.april
                    <br /> - Data Flows through multiple platform through different transformation stages. 
                    <br /> - Worked on design of python and pyspark framework to process the data from one layer to another layer of Snowflake
                    <br /> - Worked on financial Statistical Models to convert the code from SAS to Python
                    <br /> - Worked on data analysis part to identify the huge data gaps from history data and fix data issues.
                    <br /> - Created multiple Snowflake SQL scripts for ETL process
                    <br /> - Created Snowflake tasks to schedule the data loads in snowflake
                    <br /> - Worked on adhoc requests from LOB to identify the data gaps and issues in existing procedures.
                    <br /> - Experience on Business communications and demo’s 
                    <br /> - Worked on JILs AutoSys, so daily load can be resumed.
                    <br /> - Worked with Netezza database to migrate large datasets to Snowflake prod.

                </li>
                <li>
                    <strong>Data Engineer</strong> - FIS Global (Aug 2021 – Dec 2023)  
                    <br /> - Designed data models and developed the Snowflake pipeline to ingest the data.
                    <br /> - Analyzed data for bringing in new Domains data as part of reporting project.
                    <br /> - Created multiple virtual warehouses, for data load purposes.
                    <br /> - Worked on data analysis part to identify the huge data gaps from Hive and Snowflake database
                    <br /> - Created multiple Snowflake SQL and JavaScript procedure to define the ETL process
                    <br /> - Created Snowflake tasks to schedule the data loads in snowflake
                    <br /> - Worked and implemented the Salesforce procedures created the dashboard from the analyzed data.
                    <br /> - Created logic for benchmark data and aggregated data tables, to see how current bank is doing compared to other banks 
                    <br /> - Worked on different domains such as Revenue, Settlement, Disputes, Salesforce, Revenue Share and Interchange.
                    <br /> - Worked on adhoc requests from LOB to identify the data gaps and issues in existing procedures.
                    <br /> - Experience on Business communications and demo’s 
                    <br /> - Worked on to synthesize the data to created demo dashboards for new customers.
                    <br /> - Tested some of the components as part of designed dashboard and data related issues.
                    <br /> - Worked on Impala database to query multiple tables to extract data.
                    <br /> - Working experience on DataStage 11.7 parallel and Sequence jobs 
                </li>
                <li>
                    <strong>Data Engineer</strong> - DTCC (March 2021 – Aug 2021)    	               					
                    <br /> - Design, Develop and implement processes and controls for incremental loads using IBM DataStage 11.5 and 11.7
                    <br /> - Extracted data from disparate source systems such as Oracle, Snowflake, and Files (.csv, .dat).
                    <br /> - Used multiple stages like Transformer, oracle, snowflake, join stage, look-up stage, etc for Data Loading and transformations.
                    <br /> - Design and develop IT solutions based on client’s system requirement. Construct and test IT components based on the identified needs of the organization.
                    <br /> - Create Data Warehouse to load data from source for multiple dimensional and fact tables with Snowflake Database. 
                    <br /> - Used Dataiku for reading and writing data from snowflake tables
                    <br /> - Automated all data access, transformation and data pipelines on snowflake using Dataoku deployment.
                    <br /> - Design, Develop and Implement process and controls for incremental loads using IBM DataStage 11.5 and 11.7
                    <br /> - DataStage project migration using Gitlab and Jenkins tool.
                    <br /> - Designed high level ETL and ELT architecture for overall data transfer from the source server to Enterprise services.
                    <br /> - Worked on the DataStage ETL jobs to source data from different sources such as oracle, CSV/TXT, and Snowflake data.
                    <br /> - Apply Transformation on the data extracted as per business requirement and add additional audit columns.
                    <br /> - Used multiple stages like Transformer, Oracle connector, Snowflake connector, join stage, look up stage, filter, sort and CDC stage for transformation and loading.
                    <br /> - Created Parameterized Sequencer and Parallel Reusable jobs to extract and load data using parameter sets.
                    <br /> - Designed Scripts and Control-M Cycles for scheduling purposes.
                    <br /> - Responsible for end-to-end verification of requirements gathered and the functional specifications and come up with technical design documents and Source to Target mappings documents.
                    <br /> - Responsible to analyze and fix issues in the production environment on a priority basis.
                    <br /> - Weekly communications with the ETL Community group to provide design reviews and demos.
                    <br /> - Used Jira for monitoring work completion and communications with product owners for requirement analysis.
                    <br /> - Used AutoSys tool for production monitoring and scheduling purposes. 
                </li>
                <li>
                    <strong>DataStage Developer</strong> - United Service Automobile Association (USAA) (Feb 2020 – Aug-2021)
                    <br /> - Design, Developed and implemented processes and controls for incremental loads using IBM Datastage
                    <br /> - Extracted data from disparate source systems such as Oracle, Hive, Snowflake and Files (CSV).
                    <br /> - Applied transformations on the extracted data as required business requirement, added additional fields.
                    <br /> - Utilized transformation stages like Transformer, oracle, snowflake, join, look-up, etc for data processing.
                    <br /> - Designed and developed IT solutions based on client’s system requirement. 
                    <br /> - used Gitlab from DS11.5 to DS11.7 test and prod for DataStage migrations.
                    <br /> - Designed high level ETL and ELT architecture for overall data transfer from the source server to Enterprise services.
                    <br /> - Worked on the DataStage ETL jobs to source data from different sources such as oracle, db2, CSV/TXT, and Json/XML and unstructured files.
                    <br /> - Apply Transformation on the data extracted as per business requirement and add additional audit columns.
                    <br /> - Used multiple stages like Transformer, Oracle connector, Snowflake connector, join stage, look up stage, filter, sort and CDC stage for transformation and loading.
                    <br /> - Created Parameterized Sequencer and Parallel Reusable jobs  to extract and load data using parameter sets.
                    <br /> - Designed Scripts and Control-M Cycles for scheduling purposes.
                    <br /> - Experience in loading data into snowflake by using a copy statement for bulk load.
                    <br /> - Experience working on Snowpipe for pulling data into snowflake tables from different sources for continuous loading.
                    <br /> - Currently working on migrating snowflake from oracle, designed jobs to load data to snowflake by extracting from oracle DB.
                    <br /> - Experience working with Hive and HDFS to process files.
                    <br /> - Responsible for end-to-end verification of requirements gathered and the functional specifications and come up with technical design documents and Source to Target mappings documents.
                    <br /> - Responsible to analyze and fix issues in the production environment on a priority basis.
                    <br /> - Weekly communications with the ETL Community group to provide design reviews and demos.
                    <br /> - Used Jira for monitoring work completion and communications with product owners for requirement analysis.
                </li>   
                <li>
                    <strong>DataStage Developer</strong> - Gap (Nov 2019 – Feb-2020)
                    <br /> - Prepared on Source to Target mapping documents
                    <br /> - Understanding change requirements and getting approval from the business to make changes in the Prod cycles
                    <br /> - Designed datastage jobs for migration to load data to Snowflake extracted from Oracle and DB2.
                    <br /> - Automated snow pipes for handling JSON files from AWS storage to load into Snowflakes on receiving vendor receipts and contracts information.
                    <br /> - Worked on Cloning & Data Shares while doing Unit Testing and share the results with Business Users
                    <br /> - Handling outbound service process using DataStage to extract multiple Purchasing Orders (POs), Receipts and other information.
                    <br /> - Built Reusable ETL frameworks for Source to Staging table load using Runtime Column Propagation
                    <br /> - Development and review of mappings involving extracting data from Flat Files, Excel files, Oracle, SQL Server, DB2 sources to Oracle database.
                    <br /> - Design the database modeling and reporting needs of the organization
                    <br /> - Performing Code Reviews and Tuning of the implemented Jobs using Resource Estimation Wizard
                    <br /> - Extensively work on IBM DataStage tools, Oracle, UDB-DB2,
                    <br /> - Involved in customer communications daily.
                    <br /> - Monitoring and handling failed cycles through CAWA scheduler tool
                    <br /> - Responsible to analyze and fix issues in the production environment on a priority basis.
                    <br /> - Maintaining the system outage logs and job failure logs for future references.
                    
                </li>    -->


            </ul>
        </section>

        <section>
            <h2>Education</h2>
            <ul>
                <li>
                    <strong>Masters of Computer Science</strong> - University of Massachusetts Dartmouth (2019)  
                    <br /> - Master thesis on creating Trajectories and analyzing vessels data using Python and PySpark  
                </li>
                <li>
                    <strong>Bachelors of Computer Science</strong> - SZABIST Karachi (2013)  
                    <br /> - Design and implmentation of remote control car using Arduino and Android app.   
                </li>
            </ul>
        </section>

        <section>
            <h2>Skills</h2>
            <ul class="skills-list">
                <li><strong>Snowflake: </strong> Snow SQL, Snowpipe, cloning, clustering, Time Travel, Snowpark </li>
                <li><strong>Cloud services: </strong> AWS S3, Snowflake, Azure </li>
                <li><strong>Hadoop Components: </strong> Hive, Sqoop, HDFS </li>
                <li><strong>Databases: </strong> DB2, Oracle (SQL Developer), SQL Server </li>
                <li><strong>DW-ETL Tools: </strong> IBM Infosphere DataStage 11.5,9.1.2/8.5.2/8.0.1, SAP BO Data Services, Talend Open Studio </li>
                <li><strong>Data Modeling </strong> Snowflake-schema modeling, Fact and dimension tables, Star-Schema modeling </li>
                <li><strong>Others </strong> Python, Unix Shell, Control-M and CAWA Scheduler, Service Now, SMP, Kafka, and PySpark, JavaScript, HTML, CSS </li>
            </ul>
        </section>

        <footer>
            <p>&copy; 2024 Ravi Shankar. All rights reserved.</p>
        </footer>
    </div>
</body>
</html>
